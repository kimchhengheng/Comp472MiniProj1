(a) what metric is best suited to this dataset/task and why (see step (2))
Because the number of feature and zero frequency word is changing over time we run the code,
the result of each model does not much different.
However, if we have a same feature number every time we run, the model with smoothing is better for this dataset. 
By adding the smoothing, the problem of zero probability could be tackle.
Becuase zero probability would cause the muliplication become zero. There are many 
zero frequency world in the term-document matrix of the feature.

(b) why the performance of steps (8-10) are the same or are different than those of step (7) above.
The performance of each try is not the same.
The size of the feature or attribute would be change over time we run the program. 
Also, the word with frequency of zero is also changing over time. This would cause all the numerical 
calculation of precision, recall and others to change.